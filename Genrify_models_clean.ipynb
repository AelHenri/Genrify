{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genrify Project - Phase II\n",
    "## Music genre prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Henri Toussaint<br>\n",
    "Victor Saint Guilhem<br>\n",
    "Benoît Lafon<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project sets out to predict the genre of a music using the Spotify API, which provides audio features for each tracks. To collect the tracks, we used a recommandation function with a genre seed. We handpicked 20 genres in order to best represent tracks, and from each genre, we collected 100 tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from genrify_module import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"music_collection.csv\")\n",
    "#data = data.iloc[np.random.permutation(len(data))]\n",
    "pd_attributes = data.loc[:,'acousticness':'valence']\n",
    "attributes = np.array(pd_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Number of instances: ' + str(data.shape[0]))\n",
    "print('Number of attributes: ' + str(pd_attributes.shape[1]))\n",
    "print('Attributes:')\n",
    "for i in pd_attributes.columns.values:\n",
    "    print('\\t'+str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We chose the accuraccy measure because blablabla (à remplir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To store the performance of different models\n",
    "global_accuracy_scores = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc_attributes = scale(attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GENRES = ['alternative','blues','classical','country','electro','folk','french','hard-rock','heavy-metal','hip-hop','indie','jazz','pop','psych-rock','punk-rock','r-n-b','reggae','rock','soul','techno']\n",
    "target_multinomial = []\n",
    "for i in data['genre']:\n",
    "    target_multinomial.append(GENRES.index(i))\n",
    "target_multinomial=np.array(target_multinomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_model = DummyClassifier(strategy='uniform')\n",
    "\n",
    "random_acc_scores = cross_val_score(random_model, sc_attributes, target_multinomial,cv=10)\n",
    "avg_random_acc = np.mean(random_acc_scores)\n",
    "print(\"Averaged Accuracy: \" + str(avg_random_acc))\n",
    "global_accuracy_scores[\"Random\"] = avg_random_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "majo_model = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "majo_acc_scores = cross_val_score(majo_model, sc_attributes, target_multinomial,cv=10)\n",
    "avg_majo_acc = np.mean(majo_acc_scores)\n",
    "print(\"Averaged Accuracy: \" + str(avg_majo_acc))\n",
    "global_accuracy_scores[\"Majority Class \"] = avg_majo_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(sc_attributes, target_multinomial, train_size=0.67, random_state=1)\n",
    "\n",
    "Cs = [0.01,0.1,1,2,3,5]\n",
    "max_score = 0\n",
    "best_c = Cs[0]\n",
    "\n",
    "for c in Cs:\n",
    "    model = LogisticRegression(C=c, multi_class='multinomial', solver='newton-cg')\n",
    "    score = np.mean(cross_val_score(model, sc_attributes, target_multinomial,cv=10, scoring='accuracy'))\n",
    "    print(\"Averaged accuracy for c=\" + str(c) + \": \" + str(score) )\n",
    "    if (score > max_score):\n",
    "        max_score = score\n",
    "        best_c = c\n",
    "\n",
    "print(\"Best accuracy is \"+ str(max_score) + \" with c=\" + str(best_c))\n",
    "global_accuracy_scores[\"Logistic Regression \"] = max_score\n",
    "\n",
    "best_model =  LogisticRegression(C=best_c, multi_class='multinomial', solver='newton-cg')\n",
    "best_model_fitted = best_model.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "indices_max_weights = np.argmax(best_model_fitted.coef_, axis=1)\n",
    "indices_min_weights = np.argmin(best_model_fitted.coef_, axis=1)\n",
    "\n",
    "for index,genre in enumerate(GENRES):\n",
    "    print('%s:' % genre)\n",
    "    print('\\tBest attribute %s - weights=%0.3f' % (pd_attributes.columns.values[indices_max_weights[index]], best_model_fitted.coef_[index][indices_max_weights[index]]))\n",
    "    print('\\tWorst attribute %s - weights=%0.3f' % (pd_attributes.columns.values[indices_min_weights[index]], best_model_fitted.coef_[index][indices_min_weights[index]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the decision tree, default parameters seems to not enhance the result much. \n",
    "\n",
    "Therefore, we just launched it several times to find a maximum accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "for i in range(100):\n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    score = np.mean(cross_val_score(model, sc_attributes, target_multinomial,cv=10))\n",
    "#    print(\"Averaged accuracy: \" + str(score) )\n",
    "    if (score >= max_score):\n",
    "        max_score = score\n",
    "\n",
    "print(\"Best accuracy is \"+ str(max_score))\n",
    "global_accuracy_scores[\"Decision Tree \"] = max_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NB_model = GaussianNB()\n",
    "NB_acc_scores = cross_val_score(NB_model, sc_attributes, target_multinomial,cv=10)\n",
    "\n",
    "avg_NB_acc = np.mean(NB_acc_scores)\n",
    "print(\"Averaged Naïve Bayes Accuracy: \" + str(avg_NB_acc))\n",
    "global_accuracy_scores[\"Naive Bayes\"] = max_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Alphas = [0.1, 1, 2, 5, 10, 20]\n",
    "HLsizes = [20, 25, 30, 50, 100, 200]\n",
    "Activation_functions = ['tanh', 'identity', 'logistic', 'relu']\n",
    "default_alpha = 5\n",
    "default_size = 25\n",
    "final_score = 0\n",
    "final_alpha = 0\n",
    "final_size = 0\n",
    "\n",
    "for f in Activation_functions:\n",
    "    print(\"Using \" + f + \" as activation function:\")\n",
    "    max_score=0\n",
    "    best_alpha = default_alpha\n",
    "    best_size = default_size\n",
    "    \n",
    "    for a in Alphas:\n",
    "        model = MLPClassifier(hidden_layer_sizes=(best_size,),alpha=a,activation=f, solver='lbfgs', random_state = 1)\n",
    "        score = np.mean(cross_val_score(model, sc_attributes, target_multinomial,cv=10))\n",
    "        print(\"Averaged accuracy for alpha=\" + str(a) + \": \" + str(score) )\n",
    "        if (score >= max_score):\n",
    "            max_score = score\n",
    "            best_alpha = a\n",
    "    \n",
    "    print(\"Best accuracy is \"+ str(max_score) + \" with alpha=\" + str(best_alpha))\n",
    "    \n",
    "    max_score = 0\n",
    "    for s in HLsizes:\n",
    "        model = MLPClassifier(hidden_layer_sizes=(s,), alpha=best_alpha, activation=f, solver='lbfgs', random_state = 1)\n",
    "        score = np.mean(cross_val_score(model, sc_attributes, target_multinomial,cv=10))\n",
    "        print(\"Averaged accuracy for hidden layer size s=\" + str(s) + \": \" + str(score) )\n",
    "        if (score > max_score):\n",
    "            max_score = score\n",
    "            best_size = s\n",
    "    \n",
    "    print(\"Best accuracy is \"+ str(max_score) + \" with hidden layer size s=\" + str(best_size))\n",
    "    if (max_score > final_score):\n",
    "        final_score = max_score\n",
    "        final_alpha = best_alpha\n",
    "        final_size = best_size\n",
    "\n",
    "print(\"\\nThe best final accuracy is \" + str(final_score) + \" with a hidden layer size of s=\" + str(final_size) + \" and alpha=\" + str(final_alpha)+ \".\\n\")\n",
    "global_accuracy_scores[\"Neural Networks\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Cs = [0.01,0.1,1,2,3,5]\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "for k in kernels:\n",
    "    print(\"Using \"+ k + \" kernel\")\n",
    "    max_score = 0\n",
    "    best_c = Cs[0]\n",
    "    for c in Cs:\n",
    "        model = SVC(kernel = k,C=c, random_state=1)\n",
    "        score = np.mean(cross_val_score(model, sc_attributes, target_multinomial,cv=10, scoring='accuracy'))\n",
    "        print(\"Averaged accuracy for c=\" + str(c) + \": \" + str(score) )\n",
    "        if (score > max_score):\n",
    "            max_score = score\n",
    "            best_c = c\n",
    "            \n",
    "    print(\"Best accuracy is \"+ str(max_score) + \" with c=\" + str(best_c)+\"\\n\")\n",
    "\n",
    "global_accuracy_scores[\"Support Vector Machine\"] = max_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [the documentation of scikit-learn](http://scikit-learn.org/stable/modules/ensemble.html#parameters), the main parameters to adjust when using these methods are n_estimators and max_features.\n",
    "\n",
    "Besides, it can be seen in the same page that empirical good default value for classification tasks is max_features=sqrt(n_features), which is the default value.\n",
    "\n",
    "Thus, the optimization will only be done over n_estimators.\n",
    "\n",
    "After a first loop, we saw that there is no significan enhancement after n_estimators > 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estimators = range(50, 71, 1)\n",
    "max_score = 0\n",
    "best_estimator = 1\n",
    "\n",
    "for e in estimators:\n",
    "    model = RandomForestClassifier(n_estimators=e)\n",
    "    score = np.mean(cross_val_score(model, sc_attributes, target_multinomial,cv=10))\n",
    "    print(\"Averaged accuracy for estimator=\" + str(e) + \": \" + str(score) )\n",
    "    if (score >= max_score):\n",
    "        max_score = score\n",
    "        best_estimator = e\n",
    "\n",
    "print(\"Best accuracy is \"+ str(max_score) + \" with estimator=\" + str(best_estimator))\n",
    "global_accuracy_scores[\"Random Forest\"] = max_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_accuracy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model\n",
    "\n",
    "The model that best fits our data is \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
